{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fine-tuning Dutch T5-base on CNN/Daily Mail for summarization (on TPU using HuggingFace Accelerate).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyP3GWr56UsijQzhiLwd/10V",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2ade750d60214ad28c46665b3804f864": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_04d302f607c547f0b8a3da75b35ae7a4",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0f559b217a5e48e3abc3dc05220bc402",
              "IPY_MODEL_ea2557129f9f4f039ce8a6098c7f035a",
              "IPY_MODEL_caeff0cc02dd4707a1401667e3aab050"
            ]
          }
        },
        "04d302f607c547f0b8a3da75b35ae7a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0f559b217a5e48e3abc3dc05220bc402": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3e1a695677cd41e09dedf9b696d5b8a9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Epoch: 0:   0%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9075c4775b26468696aee15975c51134"
          }
        },
        "ea2557129f9f4f039ce8a6098c7f035a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3e312950dd2144d7ab0605d62b8b0118",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 16151,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 42,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_306c333fdd2241b598fc05161649add1"
          }
        },
        "caeff0cc02dd4707a1401667e3aab050": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_22bdbacf862c462bbc03d3bb6c91c408",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 42/16151 [01:35&lt;5:40:09,  1.27s/it, loss=9.13]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_539a2d52941549449558166866d0aadc"
          }
        },
        "3e1a695677cd41e09dedf9b696d5b8a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9075c4775b26468696aee15975c51134": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3e312950dd2144d7ab0605d62b8b0118": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "306c333fdd2241b598fc05161649add1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "22bdbacf862c462bbc03d3bb6c91c408": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "539a2d52941549449558166866d0aadc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/T5/Fine_tuning_Dutch_T5_base_on_CNN_Daily_Mail_for_summarization_(on_TPU_using_HuggingFace_Accelerate).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPxcHSZC1XzX"
      },
      "source": [
        "## Fine-tune T5-base-dutch to perform Dutch abstractive summarization on TPU\n",
        "\n",
        "In this notebook, we are going to fine-tune a Dtuch `T5ForConditionalGeneration` model (namely `t5-base-dutch`) whose weights were the result of the [JAX/FLAX community week](https://discuss.huggingface.co/t/open-to-the-community-community-week-using-jax-flax-for-nlp-cv/7104/57) at ðŸ¤—, in PyTorch on a Dutch summarization dataset, namely the Dutch translation of the CNN/Daily Mail dataset. We are going to fine-tune on Colab's TPU using [HuggingFace Accelerate](https://github.com/huggingface/accelerate). For data preparation, we are going to use [HuggingFace Datasets](https://github.com/huggingface/datasets).\n",
        "\n",
        "Make sure to set Runtime to \"TPU\" before running this notebook ðŸ¤—.  \n",
        "\n",
        "* T5 paper: https://arxiv.org/abs/1910.10683\n",
        "* HuggingFace' T5 documentation: https://huggingface.co/transformers/master/model_doc/t5.html\n",
        "\n",
        "Resources I used to make this notebook:\n",
        "* Venelin Valkov's awesome Youtube videos, for example [this one](https://www.youtube.com/watch?v=r6XY80Z9eSA)\n",
        "* The [official HuggingFace Accelerate TPU example](https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/accelerate/simple_nlp_example.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4MWlAlQQAwH"
      },
      "source": [
        "!pip install -q transformers datasets accelerate sentencepiece\n",
        "!pip install -q cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cf3PMqPhlWz1"
      },
      "source": [
        "## Load dataset\n",
        "\n",
        "Here we load the Dutch translation of the CNN/Daily Mail dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOco3waYgyna",
        "outputId": "4b604b65-b790-4d56-f33b-e6b946dfdf6a"
      },
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"ml6team/cnn_dailymail_nl\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration default\n",
            "WARNING:datasets.builder:Reusing dataset cnn_dailymail_nl (/root/.cache/huggingface/datasets/cnn_dailymail_nl/default/0.0.0/73618cbc23f25331390bc0475f361e0531b47feee292c9cf84d396d6a3b9b608)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLqo2AmMviIT"
      },
      "source": [
        "As we can see, the dataset only has 3 splits: train, validation and test:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLPVC9zPg67W",
        "outputId": "2a5d29b8-df05-401f-e11f-8479203482f7"
      },
      "source": [
        "dataset"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['article', 'highlights', 'id'],\n",
              "        num_rows: 287113\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['article', 'highlights', 'id'],\n",
              "        num_rows: 13368\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['article', 'highlights', 'id'],\n",
              "        num_rows: 11490\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxvsUvauldNS"
      },
      "source": [
        "Let's look at an example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FA89kduhL_D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44479062-2e95-44a4-815e-26688dbf2e61"
      },
      "source": [
        "example = dataset['train'][0]\n",
        "article = example['article']\n",
        "summary = example['highlights']\n",
        "print(\"Article:\", article)\n",
        "print(\"Summary:\", summary)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Article: (CNN) -- de bewering van de Amerikaanse minister van Buitenlandse Zaken John Kerry dat terroristen \"maar ze kunnen zich niet verbergen\" na twee operaties in Afrika in het weekend is een herinnering dat Amerika's leger is steeds actiever op het continent. Het roept ook vragen op over de internationale wettigheid van dergelijke operaties, en hun lange termijn impact, vooral in zwakke Afrikaanse staten. In sommige gevallen Amerikaanse militaire engagementen in Afrika hebben al meer instabiliteit veroorzaakt in plaats van het verminderen van de risico's voor internationale vrede en veiligheid? Lees meer: Moet de VS vrezen Boko Haram? De Delta-eenheid van het Amerikaanse leger heeft de vermeende al-Qaeda-leider Abu Anas al Libi, die geboren werd nazih Abd al Hamid al Ruqhay, in LibiÃ«, is belangrijk voor de inspanningen van de VS tegen terrorisme. Een paar maanden geleden woonden president Barack Obama en voormalig president George W. Bush een herdenkingsdienst bij in Dar es Salaam op de verjaardag van de bombardementen van 1998 op de Amerikaanse ambassades in Kenia en Tanzania die meer dan 220 mensen doodden. Al-Libi was naar verluidt het brein achter deze bombardementen en is een van de Amerikaanse gezocht mannen voor de laatste 15 jaar. Een tweede Amerikaanse militaire operatie door Navy Seals in SomaliÃ« was gericht op het vangen van islamitische militante leider zei Mukhtar Abu Zubayr, die ook gaat door Ahmed Abdi Godane, de leider van Al-Shabaab, de groep die de verantwoordelijkheid voor de aanval van vorige maand op de Westgate winkelcentrum in Kenia. De operatie werd afgebroken onder zwaar gewapend vuur tijdens hun amfibische aanval. Analyse: Al-Shabaab breekt nieuwe terrein. De SomaliÃ« operatie is een herinnering aan hoe technisch moeilijk dergelijke operaties zijn en hoe nauwkeurig Amerikaanse intelligentie moet zijn. Dergelijke operaties hoewel passen in een groter plaatje van de VS en het Westen anti-terrorisme strategie in Afrika, en hoe deze slecht bestuurde, kwetsbare staten bieden havens en laboratoria voor terrorisme franchises. Luisterend naar de moeilijkheden die de Amerikaanse Marine Zeehonden tegenkwamen in SomaliÃ«, herinnerde mij eraan dat 3 oktober 20 jaar geleden 18 Amerikaanse soldaten en honderden SomaliÃ«rs stierven in een veldslag waarbij twee Black Hawk helikopters neerschoten boven Mogadishu. Dit werd destijds gezien als een van de ergste rampen in de Amerikaanse militaire geschiedenis. Het resultaat in 1993 was dat de VS en de internationale gemeenschap SomaliÃ« aan hun eigen lot hebben overgelaten -- wat meer dan twee decennia heeft geleid tot de frontlinie van internationale terrorismebestrijdingsinspanningen. Terwijl het resultaat in 1993 was de terugtrekking, VS engagement beleid onder president George W. Bush in SomaliÃ« om de Islamitische Courts Union te verwijderen eind 2006 resulteerde in diepere radicalisering en de opkomst van Al-Shabaab. Op dezelfde manier Westerse beleid ten aanzien van LibiÃ« in 2011, het interpreteren van een smalle civiele bescherming VN-mandaat voor duwen voor regimeverandering resulteerde in de instabiliteit van LibiÃ« vandaag, en de klop-on effecten in de Sahel, waaronder de radicale islamisten het vangen van Noord-Mali totdat een Franse interventie hun greep op de macht eerder in 2013. Het terrorismebestrijdingsbeleid leeft aan de rand van het internationale recht: buitengerechtelijke moorden door drones of drones zijn duidelijk omstreden en buitengewone uitleveringen -- verdachte operaties -- zijn onwettig. Bekende buitengewone uitleveringen in Afrika hebben plaatsgevonden in veel Afrikaanse landen, waaronder Algerije, Djibouti, Egypte, EthiopiÃ«, Gambia, Kenia, LibiÃ«, Malawi, Marokko, MauritaniÃ«, Tanzania, SomaliÃ«, Zuid-Afrika en Zimbabwe. De regering van Obama heeft niet publiekelijk verklaard of dergelijke operaties doorgaan. Vandaag de Amerikaanse militaire korte congres met kaarten tonen een boog van instabiliteit door islamitische terroristen uit SomaliÃ« en de Golf van Aden over de Sahel en de Sahara naar de Atlantische Oceaan en MauritaniÃ«. Analyse: Wat is er achter Nigeria's golf van geweld? De fout zou zijn om dit te beschouwen als een homogene bedreiging. Boko Haram in Nigeria is heel anders dan Al-Shabaab, dat is anders dan AQIM. Wat vergelijkbaar is is dat al deze groepen gedijen in verzwakte en slecht bestuurde staten, maar de oplossingen zijn veel duurder en lange termijn: institutionele opbouw, goed bestuur en het creÃ«ren van banen. Het opbouwen van professionele en verantwoordelijke Afrikaanse militairen is slechts een deel van de oplossing en de westerse inspanningen in SomaliÃ« ter ondersteuning van het continentale lichaam de Afrikaanse Unie hebben succes gehad in de bestrijding van Al-Shabaab en aanzienlijk verzwakken hen, ondanks de Westgate aanval in Kenia. Het is echter ook het voeren van beleid dat niet meer schade en radicaliseren verder, zoals de Amerikaanse strategie ten aanzien van SomaliÃ« in 2006 deed en misschien Amerikaanse, Franse en Britse beleid deed ten aanzien van LibiÃ« in 2011. Als we goed bestuur en de rechtsstaat als de uiteindelijke remedies willen bepleiten, moeten we onverbiddelijk manieren vinden om ervoor te zorgen dat ons beleid gebaseerd is op waarden en niet op belangen. Vergeet niet de lessen die zijn getrokken uit SomaliÃ« en LibiÃ« over de onbedoelde gevolgen van niet-interventie en interventie is belangrijk als internationale inspanningen om het terrorisme in Afrika op lange termijn te bestrijden succesvol willen zijn.\n",
            "Summary: Anti-terrorisme beleid leeft op de rand van het internationale recht, Alex Vines schrijft . Amerikaanse invallen in Afrika laten zien dat Amerika's leger is steeds actiever op het continent . Het opbouwen van professionele verantwoordelijk militair is slechts een deel van de oplossing , zegt Vines .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dreE7nRbkapT"
      },
      "source": [
        "Each example consists of an article and a corresponding summary. Easy, huh?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SlI-qdCliv2"
      },
      "source": [
        "## Encode the dataset\n",
        "\n",
        "The T5 model, like any other Transformer model, does not directly expect text as input. Rather, it expects `input_ids` and `attention_mask`. The `input_ids` are integer vocabulary indices of the tokens of the text (you can read more about them [here](https://huggingface.co/transformers/glossary.html#input-ids)). As labels, it expects the `input_ids` of the summary. \n",
        "\n",
        "Let's encode them using the tokenizer. We also prepend the input with a so-called task prefix, which the authors of T5 used when fine-tuning the model. Here, the prefix is simply \"Vat samen: \" (which is Dutch for \"Summarize: \"), followed by a long document.\n",
        "\n",
        "As we'll train the model on TPU, we pad both the inputs and targets up to the max length. If we were to train this model on GPUs, we would instead pad them up to the longest in a batch, which is more efficient in terms of memory. However, TPUs don't like that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiLdcTmkg-_o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a819c05-3160-4dec-84df-9cbd2175a15d"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"flax-community/t5-base-dutch\")\n",
        "\n",
        "prefix = \"Vat samen: \"\n",
        "max_input_length = 512\n",
        "max_target_length = 64\n",
        "\n",
        "def preprocess_examples(examples):\n",
        "  # encode the documents\n",
        "  articles = examples['article']\n",
        "  summaries = examples['highlights']\n",
        "  \n",
        "  inputs = [prefix + article for article in articles]\n",
        "  model_inputs = tokenizer(inputs, max_length=max_input_length, padding=\"max_length\", truncation=True)\n",
        "\n",
        "  # encode the summaries\n",
        "  labels = tokenizer(summaries, max_length=max_target_length, padding=\"max_length\", truncation=True).input_ids\n",
        "\n",
        "  # important: we need to replace the index of the padding tokens by -100\n",
        "  # such that they are not taken into account by the CrossEntropyLoss\n",
        "  labels_with_ignore_index = []\n",
        "  for labels_example in labels:\n",
        "    labels_example = [label if label != 0 else -100 for label in labels_example]\n",
        "    labels_with_ignore_index.append(labels_example)\n",
        "  \n",
        "  model_inputs[\"labels\"] = labels_with_ignore_index\n",
        "\n",
        "  return model_inputs"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V__AXZ1HlouX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9c2ff45-bcca-42fb-88b8-c3dba12fff00"
      },
      "source": [
        "encoded_dataset = dataset['train'].map(preprocess_examples, batched=True, remove_columns=dataset['train'].column_names)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/cnn_dailymail_nl/default/0.0.0/73618cbc23f25331390bc0475f361e0531b47feee292c9cf84d396d6a3b9b608/cache-7781fc2c9ece3ffa.arrow\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akbnGsHkrDHg"
      },
      "source": [
        "Let's verify an example, by decoding the `input_ids` back to text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UowOG2apork",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "ea8df3ce-0229-44e4-c534-61e3010dfd83"
      },
      "source": [
        "tokenizer.decode(encoded_dataset[0]['input_ids'])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Vat samen: (CNN) -- de bewering van de Amerikaanse minister van Buitenlandse Zaken John Kerry dat terroristen \"maar ze kunnen zich niet verbergen\" na twee operaties in Afrika in het weekend is een herinnering dat Amerika\\'s leger is steeds actiever op het continent. Het roept ook vragen op over de internationale wettigheid van dergelijke operaties, en hun lange termijn impact, vooral in zwakke Afrikaanse staten. In sommige gevallen Amerikaanse militaire engagementen in Afrika hebben al meer instabiliteit veroorzaakt in plaats van het verminderen van de risico\\'s voor internationale vrede en veiligheid? Lees meer: Moet de VS vrezen Boko Haram? De Delta-eenheid van het Amerikaanse leger heeft de vermeende al-Qaeda-leider Abu Anas al Libi, die geboren werd nazih Abd al Hamid al Ruqhay, in LibiÃ«, is belangrijk voor de inspanningen van de VS tegen terrorisme. Een paar maanden geleden woonden president Barack Obama en voormalig president George W. Bush een herdenkingsdienst bij in Dar es Salaam op de verjaardag van de bombardementen van 1998 op de Amerikaanse ambassades in Kenia en Tanzania die meer dan 220 mensen doodden. Al-Libi was naar verluidt het brein achter deze bombardementen en is een van de Amerikaanse gezocht mannen voor de laatste 15 jaar. Een tweede Amerikaanse militaire operatie door Navy Seals in SomaliÃ« was gericht op het vangen van islamitische militante leider zei Mukhtar Abu Zubayr, die ook gaat door Ahmed Abdi Godane, de leider van Al-Shabaab, de groep die de verantwoordelijkheid voor de aanval van vorige maand op de Westgate winkelcentrum in Kenia. De operatie werd afgebroken onder zwaar gewapend vuur tijdens hun amfibische aanval. Analyse: Al-Shabaab breekt nieuwe terrein. De SomaliÃ« operatie is een herinnering aan hoe technisch moeilijk dergelijke operaties zijn en hoe nauwkeurig Amerikaanse intelligentie moet zijn. Dergelijke operaties hoewel passen in een groter plaatje van de VS en het Westen anti-terrorisme strategie in Afrika, en hoe deze slecht bestuurde, kwetsbare staten bieden havens en laboratoria voor terrorisme franchises. Luisterend naar de moeilijkheden die de Amerikaanse Marine Zeehonden tegenkwamen in SomaliÃ«, herinnerde mij eraan dat</s>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTaV8FujxXRQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b549025c-836b-4aca-ab2f-0a54d90bc717"
      },
      "source": [
        "labels = encoded_dataset[0]['labels']\n",
        "print(labels)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[7352, 18, 23388, 1298, 4128, 14, 5, 2460, 8, 10, 760, 604, 7, 6095, 8139, 12, 13, 873, 3, 4, 295, 11, 2355, 11, 3114, 227, 192, 19, 1694, 39, 13, 1312, 15, 246, 4188, 71, 14, 10, 8140, 3, 4, 46, 3, 12869, 8, 2206, 3, 649, 5319, 15, 530, 9, 306, 8, 5, 1041, 3, 7, 222, 8139, 12, 13, 3, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69FJBxu2yGt5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "a4e59cfe-866c-4f13-965f-6de7659c9333"
      },
      "source": [
        "tokenizer.decode([x for x in labels if x != -100])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Anti-terrorisme beleid leeft op de rand van het internationale recht, Alex Vines schrijft. Amerikaanse invallen in Afrika laten zien dat Amerika's leger is steeds actiever op het continent. Het opbouwen van professionele verantwoordelijk militair is slechts een deel van de oplossing, zegt Vines </s>\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORv91RxQsiQc"
      },
      "source": [
        "Next, let's set the format to PyTorch, and split up the dataset into training and validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_IvugS6pNA4"
      },
      "source": [
        "encoded_dataset.set_format(type=\"torch\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPp0khu2zMbE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29edd93b-6c3a-4325-8366-48256820bd42"
      },
      "source": [
        "splits = encoded_dataset.train_test_split(test_size=0.1)\n",
        "train_ds = splits['train']\n",
        "val_ds = splits['test']"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached split indices for dataset at /root/.cache/huggingface/datasets/cnn_dailymail_nl/default/0.0.0/73618cbc23f25331390bc0475f361e0531b47feee292c9cf84d396d6a3b9b608/cache-09311ab6454f52a8.arrow and /root/.cache/huggingface/datasets/cnn_dailymail_nl/default/0.0.0/73618cbc23f25331390bc0475f361e0531b47feee292c9cf84d396d6a3b9b608/cache-29563f5e0d6e69cb.arrow\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VWtJG6kzpDO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bc3c068-6bdd-4cf2-f802-e691e907e65b"
      },
      "source": [
        "print(\"Number of training examples:\", len(train_ds))\n",
        "print(\"Number of validation examples:\", len(val_ds))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 258401\n",
            "Number of validation examples: 28712\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dA1SvGNazF64"
      },
      "source": [
        "We define a function to create dataloaders."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItmsPKHDuYZm"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def create_dataloaders(train_batch_size=8, eval_batch_size=32):\n",
        "    train_dataloader = DataLoader(train_ds, shuffle=True, batch_size=train_batch_size)\n",
        "    val_dataloader = DataLoader(val_ds, shuffle=False, batch_size=eval_batch_size)\n",
        "    \n",
        "    return train_dataloader, val_dataloader"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuj0Cj0WpIQJ"
      },
      "source": [
        "## Fine-tune a model \n",
        "\n",
        "Below, we define a `training_function`, which defines a regular training loop in native PyTorch. We only need to add a few lines to make sure the code will run on TPU. The Accelerator object will take care of that.\n",
        "\n",
        "We also define a dictionary of training-related hyperparameters, which we can easily tweak."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3Mn8ziqr0wu"
      },
      "source": [
        "hyperparameters = {\n",
        "    \"learning_rate\": 0.0001,\n",
        "    \"num_epochs\": 1, # previously set to 1000\n",
        "    \"train_batch_size\": 2, # Actual batch size will this x 8 (was 8 before)\n",
        "    \"eval_batch_size\": 2, # Actual batch size will this x 8 (was 32 before)\n",
        "    \"seed\": 42,\n",
        "    \"patience\": 3, # early stopping\n",
        "    \"output_dir\": \"/content/\",\n",
        "}"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEvLESbulv-b"
      },
      "source": [
        "import torch\n",
        "from transformers import T5ForConditionalGeneration, AdamW, set_seed\n",
        "from accelerate import Accelerator\n",
        "from tqdm.notebook import tqdm\n",
        "import datasets\n",
        "import transformers\n",
        "\n",
        "def training_function():\n",
        "    # Initialize accelerator\n",
        "    accelerator = Accelerator()\n",
        "\n",
        "    # To have only one message (and not 8) per logs of Transformers or Datasets, we set the logging verbosity\n",
        "    # to INFO for the main process only.\n",
        "    if accelerator.is_main_process:\n",
        "        datasets.utils.logging.set_verbosity_warning()\n",
        "        transformers.utils.logging.set_verbosity_info()\n",
        "    else:\n",
        "        datasets.utils.logging.set_verbosity_error()\n",
        "        transformers.utils.logging.set_verbosity_error()\n",
        "\n",
        "    # The seed need to be set before we instantiate the model, as it will determine the random head.\n",
        "    set_seed(hyperparameters[\"seed\"])\n",
        "\n",
        "    # Instantiate the model, let Accelerate handle the device placement.\n",
        "    model = T5ForConditionalGeneration.from_pretrained(\"flax-community/t5-base-dutch\")\n",
        "\n",
        "    # Instantiate optimizer\n",
        "    optimizer = AdamW(model.parameters(), lr=hyperparameters[\"learning_rate\"])\n",
        "\n",
        "    # Prepare everything\n",
        "    train_dataloader, val_dataloader = create_dataloaders(\n",
        "        train_batch_size=hyperparameters[\"train_batch_size\"], eval_batch_size=hyperparameters[\"eval_batch_size\"]\n",
        "    )\n",
        "    # There is no specific order to remember, we just need to unpack the objects in the same order we gave them to the\n",
        "    # prepare method.\n",
        "    model, optimizer, train_dataloader, val_dataloader = accelerator.prepare(model, optimizer, \n",
        "                                                                             train_dataloader, val_dataloader)\n",
        "    \n",
        "    # Now we train the model\n",
        "    epochs_no_improve = 0\n",
        "    min_val_loss = 1000000\n",
        "    for epoch in range(hyperparameters[\"num_epochs\"]):\n",
        "        # We only enable the progress bar on the main process to avoid having 8 progress bars.\n",
        "        progress_bar = tqdm(range(len(train_dataloader)), disable=not accelerator.is_main_process)\n",
        "        progress_bar.set_description(f\"Epoch: {epoch}\")\n",
        "        model.train()\n",
        "        for batch in train_dataloader:\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss\n",
        "            accelerator.backward(loss)\n",
        "            \n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            progress_bar.set_postfix({'loss': loss.item()})\n",
        "            progress_bar.update(1)\n",
        "\n",
        "        # Evaluate at the end of the epoch (distributed evaluation as we have 8 TPU cores)\n",
        "        model.eval()\n",
        "        validation_losses = []\n",
        "        for batch in val_dataloader:\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**batch)\n",
        "            loss = outputs.loss\n",
        "\n",
        "            # We gather the loss from the 8 TPU cores to have them all.\n",
        "            validation_losses.append(accelerator.gather(loss[None]))\n",
        "\n",
        "        # Use accelerator.print to print only on the main process.\n",
        "        val_loss = torch.stack(validation_losses).sum().item() / len(validation_losses)\n",
        "        accelerator.print(f\"epoch {epoch}: validation loss:\", val_loss)\n",
        "        if val_loss < min_val_loss:\n",
        "          epochs_no_improve = 0\n",
        "          min_val_loss = val_loss\n",
        "          continue\n",
        "        else:\n",
        "          epochs_no_improve += 1\n",
        "          # Check early stopping condition\n",
        "          if epochs_no_improve == hyperparameters[\"patience\"]:\n",
        "            print(\"Early stopping!\")\n",
        "            break\n",
        "\n",
        "    # save trained model\n",
        "    accelerator.wait_for_everyone()\n",
        "    unwrapped_model = accelerator.unwrap_model(model)\n",
        "    # Use accelerator.save to save\n",
        "    unwrapped_model.save_pretrained(hyperparameters[\"output_dir\"], save_function=accelerator.save)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFv3QFEVF16U"
      },
      "source": [
        "Next, we can easily start training by wrapping the `training_function` in a `notebook_launcher`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyFsYRCOrp-g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2ade750d60214ad28c46665b3804f864",
            "04d302f607c547f0b8a3da75b35ae7a4",
            "0f559b217a5e48e3abc3dc05220bc402",
            "ea2557129f9f4f039ce8a6098c7f035a",
            "caeff0cc02dd4707a1401667e3aab050",
            "3e1a695677cd41e09dedf9b696d5b8a9",
            "9075c4775b26468696aee15975c51134",
            "3e312950dd2144d7ab0605d62b8b0118",
            "306c333fdd2241b598fc05161649add1",
            "22bdbacf862c462bbc03d3bb6c91c408",
            "539a2d52941549449558166866d0aadc"
          ]
        },
        "outputId": "09f1b37c-b7db-4533-aec9-204e983a033c"
      },
      "source": [
        "from accelerate import notebook_launcher\n",
        "\n",
        "notebook_launcher(training_function)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Launching a training on 8 TPU cores.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loading configuration file https://huggingface.co/flax-community/t5-base-dutch/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/429863f709b0983f0f5730aa35d1b92a471bc989bd44b2de2a0b4a21cac1a00a.3002a7d60c9ed2cc52b423329227d69c542ddfb8997267a81d19761eac5e8ed6\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \".\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 3072,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.9.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32103\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/flax-community/t5-base-dutch/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/01e919204b8eb6670f3c8e0afe3d2ca093db218521334bd462b75d571a1de4e5.de6f5e63b55c7f65ccb2c1b0f5d427d683a2de494e7448429ae23bb3e59976ba\n",
            "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
            "\n",
            "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at flax-community/t5-base-dutch.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2ade750d60214ad28c46665b3804f864",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/16151 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmExPYic6EBL"
      },
      "source": [
        "## Inference\n",
        "\n",
        "Now that we have a trained model, let's use it to generate a summary on a new, unseen text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yd0La4YY4ixe"
      },
      "source": [
        "text = \"\"\"De Rode Duivels moeten hun droom op een Europese titel opbergen. \n",
        "De Belgen verloren vrijdag in MÃ¼nchen in de kwartfinales van een uitgekookt ItaliÃ« met 2-1.\n",
        "BelgiÃ« kwam via Barella en Insigne op een dubbele achterstand, maar Lukaku gaf nieuwe hoop \n",
        "met een strafschop. De gelijkmaker zat er echter niet meer in.\n",
        "De Italianen begonnen het beste aan de wedstrijd, al bleven grote kansen uit. Even voor het \n",
        "kwartier leek Bonucci uit het niets na een vrije trap met de buik de score te openen, \n",
        "maar de videoref keurde de goal af voor buitenspel.\n",
        "De Belgen moesten het evenwicht herstellen. De Bruyne waagde zijn kans met een afstandschot, dat \n",
        "Donnarumma geweldig met de vlakke hand uit doel ranselde. Diezelfde Donnarumma moest \n",
        "even nadien Lukaku, na een nieuwe tegenaanval, van de 1-0 houden. De match ging goed op en af - \n",
        "de Italianen hadden het meeste balbezit, maar de Belgen loerden onder impuls van De Bruyne \n",
        "op de counter.\"\"\"\n",
        "\n",
        "trained_model = T5ForConditionalGeneration.from_pretrained(hyperparameters[\"output_dir\"])\n",
        "\n",
        "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
        " \n",
        "generated_ids = trained_model.generate(input_ids, do_sample=True, \n",
        "    max_length=50, \n",
        "    top_k=0, \n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "summary = tokenizer.decode(generated_ids.squeeze(), skip_special_tokens=True)\n",
        "print(summary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIkZRa3hHdd4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}